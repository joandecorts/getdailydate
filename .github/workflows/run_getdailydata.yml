name: run-getdailydata

on:
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: getdailydate-run
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run scraper
        run: |
          python full_scraper_resum_diari_final.py

      - name: Collect outputs into repo
        run: |
          echo "GITHUB_WORKSPACE=$GITHUB_WORKSPACE"
          OUTSIDE_DIR="$(dirname "$GITHUB_WORKSPACE")/src/data"
          INSIDE_DIR="$GITHUB_WORKSPACE/src/data"

          echo "OUTSIDE_DIR=$OUTSIDE_DIR"
          echo "INSIDE_DIR=$INSIDE_DIR"

          mkdir -p "$INSIDE_DIR"

          if [ -d "$OUTSIDE_DIR" ]; then
            cp -f "$OUTSIDE_DIR"/* "$INSIDE_DIR"/ || true
            echo "Contents inside repo after copy:"
            ls -la "$INSIDE_DIR"
          else
            echo "Outside dir not found: $OUTSIDE_DIR"
            exit 1
          fi

      - name: Commit outputs (if any)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Evitar soroll
          echo "__pycache__/" >> .gitignore || true
          echo "*.pyc" >> .gitignore || true

          # Afegeix outputs
          git add src/data/ .gitignore || true

          git commit -m "Update daily data" || echo "No changes"

          # Evitar 'fetch first'
          git pull --rebase origin main
          git push
